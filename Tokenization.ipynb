{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokeniztion- A process in which a paragraph is converted into sentences or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Everyone my name is Md Aftab And I am fron Araria.\n",
      "i am a 3rd year student of B.Tech in Computer Science and Engineering.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus=\"\"\"Hello Everyone my name is Md Aftab And I am fron Araria.\n",
    "i am a 3rd year student of B.Tech in Computer Science and Engineering.\n",
    "\"\"\"\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types of Stemming in NLP**  \n",
    "\n",
    "Stemming algorithms use different approaches to reduce words to their root forms. Below are the most common types of stemming algorithms:  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Porter Stemmer** (Most Common)  \n",
    "- Developed by **Martin Porter** in 1980.  \n",
    "- Uses a **set of rules** to remove common suffixes (e.g., \"ing\", \"ed\", \"s\").  \n",
    "- Produces **non-dictionary words** sometimes.  \n",
    "- Good for general-purpose NLP tasks.  \n",
    "\n",
    "**Example:**  \n",
    "- \"running\" â†’ \"run\"  \n",
    "- \"flies\" â†’ \"fli\"  \n",
    "- \"happiness\" â†’ \"happi\"  \n",
    "\n",
    "\n",
    "\n",
    "### **2. Snowball Stemmer** (Improved Porter Stemmer)  \n",
    "- Also called **\"Porter2\"**, an improved version of the Porter Stemmer.  \n",
    "- **Supports multiple languages** (English, Spanish, French, etc.).  \n",
    "- More accurate than the Porter Stemmer.  \n",
    "\n",
    "**Example:**  \n",
    "- \"running\" â†’ \"run\"  \n",
    "- \"happily\" â†’ \"happi\"  \n",
    "\n",
    "\n",
    "### **3. Lancaster Stemmer** (More Aggressive)  \n",
    "- More **aggressive** than Porter and Snowball.  \n",
    "- Can **over-stem** (reduce words too much).  \n",
    "- Faster but less accurate.  \n",
    "\n",
    "**Example:**  \n",
    "- \"running\" â†’ \"run\"  \n",
    "- \"happiness\" â†’ \"happy\"  \n",
    "- \"organization\" â†’ \"organ\"  \n",
    "\n",
    "\n",
    "### **4. Regex-based Stemmer (Regular Expression Stemmer)**  \n",
    "- Uses **regular expressions** to define stemming rules.  \n",
    "- Good for **custom stemming** when specific suffixes need to be removed.  \n",
    "\n",
    "**Example:**  \n",
    "- Removing **common suffixes** like \"ing\", \"ed\", \"ly\".  \n",
    "\n",
    "\n",
    "\n",
    "### **5. Krovetz Stemmer**  \n",
    "- A **lightweight stemmer**, focuses on **minimizing over-stemming**.  \n",
    "- Uses a **dictionary lookup** to ensure valid words.  \n",
    "- Less aggressive than Porter, Snowball, and Lancaster.  \n",
    "\n",
    "**Example:**  \n",
    "- \"running\" â†’ \"run\"  \n",
    "- \"cats\" â†’ \"cat\"  \n",
    "- \"better\" â†’ \"better\" (Keeps correct base form)  \n",
    "\n",
    "> **Note:** Krovetz Stemmer is mainly used in **Information Retrieval (IR)** systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison of Stemmers**  \n",
    "\n",
    "| **Stemmer**       | **Aggressiveness** | **Accuracy** | **Supports Multiple Languages?** | **Common Use Cases** |\n",
    "|------------------|-----------------|------------|----------------------------|----------------|\n",
    "| **Porter**       | Medium          | Medium     | No                         | General NLP |\n",
    "| **Snowball**     | Medium          | High      | Yes                        | Multilingual NLP |\n",
    "| **Lancaster**    | High (Over-stemming) | Low   | No                         | Fast Stemming |\n",
    "| **Regex**        | Customizable    | Varies     | No                         | Simple, Rule-Based NLP |\n",
    "| **Krovetz**      | Low             | High      | No                         | Information Retrieval |\n",
    "\n",
    "Would you like an example of using **different stemmers together**? ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Everyone my name is Md Aftab And I am fron Araria.\n",
    "i am a 3rd year student of B.Tech in Computer Science and Engineering.\n",
    "\"\"\"\n",
    "\n",
    "docs=sent_tokenize(corpus) ## making two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Everyone my name is Md Aftab And I am fron Araria.',\n",
       " 'i am a 3rd year student of B.Tech in Computer Science and Engineering.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting paragraph into words.\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Everyone',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Md',\n",
       " 'Aftab',\n",
       " 'And',\n",
       " 'I',\n",
       " 'am',\n",
       " 'fron',\n",
       " 'Araria',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'a',\n",
       " '3rd',\n",
       " 'year',\n",
       " 'student',\n",
       " 'of',\n",
       " 'B.Tech',\n",
       " 'in',\n",
       " 'Computer',\n",
       " 'Science',\n",
       " 'and',\n",
       " 'Engineering',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Everyone', 'my', 'name', 'is', 'Md', 'Aftab', 'And', 'I', 'am', 'fron', 'Araria', '.']\n",
      "['i', 'am', 'a', '3rd', 'year', 'student', 'of', 'B.Tech', 'in', 'Computer', 'Science', 'and', 'Engineering', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentences in docs:\n",
    "    print(word_tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tree Bank Word Tokenizer.\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tbwt=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Everyone',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Md',\n",
       " 'Aftab',\n",
       " 'And',\n",
       " 'I',\n",
       " 'am',\n",
       " 'fron',\n",
       " 'Araria.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'a',\n",
       " '3rd',\n",
       " 'year',\n",
       " 'student',\n",
       " 'of',\n",
       " 'B.Tech',\n",
       " 'in',\n",
       " 'Computer',\n",
       " 'Science',\n",
       " 'and',\n",
       " 'Engineering',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbwt.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"write\",\"written\",\"programming\",\"program\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "write----->write\n",
      "written----->written\n",
      "programming----->program\n",
      "program----->program\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"----->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regexp Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=RegexpStemmer(regexp='ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TAbl'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('Table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "print(ls.stem(\"organization\"))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
